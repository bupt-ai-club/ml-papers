# ML Papers第1期

## Solving olympiad geometry without human demonstrations

**作者:** Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He & Thang Luong

**发表刊物/会议：** Nature

**发表年份：** 2024

**论文地址：** https://www.nature.com/articles/s41586-023-06747-5

**代码地址：** 无

### 内容概要

AlphaGeometry——一个充当定理证明者的人工智能系统，无需人类演示即可解决奥林匹克几何问题；该系统接受了涉及不同复杂程度的数百万定理和证明的合成数据的训练；这些数据用于训练神经语言模型，该模型可以解决奥林匹克级别的问题并接近国际数学奥林匹克（IMO）金牌得主的平均表现。

![](pp001/AlphaGeometry.png)

这篇论文介绍了一个名为AlphaGeometry的定理证明器，它专门用于解决欧几里得平面几何问题。AlphaGeometry是一个神经符号系统，它通过合成数百万不同复杂度的定理和证明来绕过人类证明的翻译需求。这个系统在解决奥林匹克数学竞赛（IMO）级别的几何问题上表现出色，解决了30个最新问题中的25个，超过了之前最好的方法，后者只解决了10个问题，并且接近了IMO金牌得主的平均表现。值得注意的是，AlphaGeometry能够产生人类可读的证明，并且在IMO 2000和2015年的所有几何问题上通过了人类专家的评估，还发现了2004年IMO定理的一个广义版本。

AlphaGeometry的核心是一个神经语言模型，该模型从头开始在大规模合成数据上进行训练，以指导符号推理引擎在具有挑战性的问题中通过无限分支点。这个系统结合了语言模型和符号引擎，通过交替运行来生成证明。在证明搜索过程中，语言模型生成辅助构造，而符号引擎则负责执行高效的符号推理。这种方法在几何定理证明领域取得了显著的进步，尤其是在辅助构造方面，这是证明许多困难定理的关键挑战。

论文还讨论了AlphaGeometry在其他数学领域的适用性，提出了一个通用的指导框架，并讨论了如何将这种方法应用于其他领域。此外，论文还提供了关于AlphaGeometry在不同测试集上的性能分析，包括在减少训练数据和搜索深度时的表现，以及在更大规模的231个几何问题测试集上的表现。

## Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering

**作者:** Tal Ridnik, Dedy Kredo, Itamar Friedman

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.08500

**代码地址：** https://github.com/Codium-ai/AlphaCodium

### 内容概要

AlphaCodium - 一种面向代码的迭代流程，可改进代码生成的LLMs；它涉及提高LLMs中的代码生成能力的两个关键步骤：i）额外生成的数据（问题自我反思和测试推理）以帮助迭代过程，ii）使用额外的人工智能生成丰富公共测试测试；使用 CodeContests 验证数据集，GPT-4 pass@5 准确率从使用单个精心设计的提示的 19% 提高到使用 AlphaCodium 流程的 44%；它甚至优于 AlphaCode，使用的计算预算要小得多，LLM 调用要少 4 个数量级。

![](pp001/AlphaCodium.png)

 这篇论文介绍了一个名为AlphaCodium的代码生成方法，它针对大型语言模型（LLMs）在解决编程竞赛问题时的性能进行了优化。AlphaCodium是一个基于测试的、多阶段的、代码导向的迭代流程，旨在提高LLMs在代码问题上的表现。该方法在CodeContests数据集上进行了测试，这是一个包含来自Codeforces等编程竞赛平台的挑战性代码生成数据集。

AlphaCodium的核心思想是将问题解决过程分为两个主要阶段：预处理阶段和代码迭代阶段。在预处理阶段，AlphaCodium通过自然语言理解问题，而在代码迭代阶段，它通过迭代地运行和修复生成的代码来解决特定的测试。这种方法包括生成额外的数据（如问题反思和测试推理）来辅助迭代过程，以及通过AI生成的测试来丰富公共测试。

论文中提到，与直接提示相比，AlphaCodium流程在各种模型上都显著提高了性能。例如，对于GPT-4模型，在验证集上的准确率（pass@5）从19%提高到了44%。AlphaCodium还优于之前的工作，如AlphaCode和CodeChain，同时具有显著较小的计算预算。

AlphaCodium采用了一些特定的设计概念和最佳实践，包括使用YAML结构化输出、生成模块化代码、通过项目符号列表分析进行语义推理、软决策与双重验证、鼓励探索和使用测试锚点等。

## RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture

**作者:** Angels Balaguer, Vinamra Benara, Renato Cunha, Roberto Estevão, Todd Hendry, Daniel Holstein,
Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp,
Bruno Silva, Swati Sharma, Vijay Aski, Ranveer Chandra

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.08406

**代码地址：** 无

### 内容概要

RAG与微调 - 报告讨论了在使用像Llama 2和GPT-4这样的大型语言模型（LLMs）时，RAG和微调之间的权衡；对在农业数据集上应用这些流程进行了详细分析，并突出了洞察；观察到在微调模型时准确率增加了超过6个百分点，这与RAG的累积效果相结合，进一步增加了5个百分点的准确率。

![](pp001/RAGvsFine-tuning.png)

文章提出了一个包含多个阶段的流程，用于微调和RAG，并针对多个流行的LLMs（包括Llama2-13B、GPT-3.5和GPT-4）展示了这两种方法的权衡。流程包括从PDF中提取信息、生成问题和答案、用于微调以及利用GPT-4评估结果。文章提出了评估RAG和微调流程不同阶段性能的指标，并在农业数据集上进行了深入研究。研究结果表明，微调模型在准确性上提高了超过6个百分点，并且与RAG相结合，进一步提高了5个百分点的准确性。在一项特定实验中，微调模型能够利用地理信息回答具体问题，将答案相似度从47%提高到72%。总体而言，研究结果表明，使用LLMs构建的系统可以适应并整合特定行业的关键知识，为LLMs在其他工业领域的进一步应用铺平了道路。

## Self-Rewarding Language Models

**作者:** Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.10020

**代码地址：** 无

### 内容概要

自我奖励模型 - 提出了一种自我奖励方法，该方法使用模型本身作为法官提示在训练期间提供奖励；使用从自指令创建阶段生成的数据构建的偏好对，迭代 DPO 进行指令跟随训练；使用这种方法，在三次迭代中微调 Llama 2 70B 模型可以得到一个优于 LLMs 的模型，例如 AlpacaEval 2.0 排行榜上的 Claude 2 和 Gemini Pro。

![](pp001/Self-Rewarding.png)

为了实现超越人类的智能体，未来的模型需要超越人类的反馈来提供足够的训练信号。现有的方法通常从人类偏好中训练奖励模型，这可能会受到人类性能水平的限制，并且这些单独的冻结奖励模型在大型语言模型（LLM）训练期间无法改进。为了解决这个问题，作者研究了自我奖励语言模型（Self-Rewarding Language Models），这种模型在训练过程中通过LLM-as-a-Judge提示来自动生成奖励。

主要贡献和发现包括：

1. **自我奖励语言模型**：提出了一种新型的智能体，它既是指令遵循模型（生成给定提示的响应），又能生成和评估新的指令遵循示例以添加到自己的训练集中。这种模型使用迭代直接偏好优化（Iterative DPO）框架进行训练。

2. **迭代训练**：从种子模型开始，模型在每次迭代中都会创建新的训练数据，这些数据基于模型自己生成的响应和奖励。这个过程可以迭代进行，每次迭代都旨在提高模型的性能。

3. **实验结果**：通过在Llama 2 70B模型上进行三次迭代训练，得到的模型在AlpacaEval 2.0排行榜上的表现超过了包括Claude 2、Gemini Pro和GPT-4 0613在内的许多现有系统。

4. **自我奖励的优势**：自我奖励模型不仅提高了遵循指令的能力，还提高了生成高质量奖励的能力。这意味着模型在迭代训练中能够为自己提供比前一次迭代更高质量的偏好数据集。

5. **研究前景**：这项工作为模型在两个维度上持续改进的可能性打开了大门，即模型能够不断改进其遵循指令的能力和奖励建模能力。

论文还讨论了自我奖励语言模型的局限性，包括需要进一步评估和理解迭代训练的极限。作者提出了未来的研究方向，包括探索更多的迭代次数、不同语言模型的能力以及安全性评估。此外，作者还强调了在自我奖励训练过程中使用LLM-as-a-Judge程序来评估安全性的潜力。

## Tuning Language Models by Proxy

**作者:** Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A. Smith

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.08565

**代码地址：** 无

### 内容概要

通过代理调整语言模型 - 引入代理调整，这是一种解码时算法，它利用小基础模型和微调基础模型之间的 logits 差异来修改目标 LLM 的 logits；这可以使更大的目标基础模型能够像其微调版本一样执行；代理调整应用于 Llama2-70B，使用仅 7B 大小的代理，以缩小 Llama2-70B 与其调整后的chat版本之间 88% 的差距。

![](pp001/Tuning-Language-Models-by-Proxy.png)

有一种新的有前途的方法可以在不修改权重的情况下微调 LLMs ，称为代理调整。
它是如何工作的？这是一种简单的解码时方法，您可以在其中修改目标 LLM 的 logits。特别是，您可以计算较小的基础模型和微调模型之间的 logits 差异，然后将该差异应用于目标模型的 logits。

更具体地说，假设目标是改进大目标模型 (M1)。

主要思想是采用两个小模型：
- 小型基础型号（M2）
- 微调的基础模型 (M3)

然后，您只需将较小模型的预测（输出词汇表的对数）的差异应用于目标模型 M1。

改进后的目标模型的输出计算公式为 M1*(x) = M1(x) + [M3(x) - M2(x)]

根据实验结果，这种方法效果出奇的好。作者对此进行了测试
A. 指令调优
B. 领域适应
C. 针对特定任务的微调

为了简洁起见，只关注A点，下面是一个具体的例子：

1) 目标是将 Llama 2 70B Base 模型改进到 Llama 2 70B Chat 的水平，但不执行任何 RLHF 从 Base -> Chat 获取模型。

2) 他们采用了小 10 倍的 Llama 2 7B 模型，并对其进行了指令微调。

3) 微调后，他们计算了 7B Base 和 7B Finetuned 之间输出词汇的 logits 差异

4) 他们将 3) 的差异应用到 Llama 2 70B 基础模型。这使得 70B Base 模型的性能非常接近 70B Chat。

当然，这种方法唯一的警告是，较小的模型必须使用与较大模型相同的词汇进行训练。理论上，如果人们了解 GPT-4 词汇表并能够访问其 logit 输出，则可以使用这种方法创建新的专用 GPT-4 模型。

## Reasoning with Reinforced Fine-Tuning 

**作者:** Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.08967

**代码地址：** https://github.com/lqtrung1998/mwp_ReFT

### 内容概要

强化微调推理 - 提出了一种方法 ReFT，以增强 LLMs 推理的泛化性；它从应用 SFT 开始，然后应用在线 RL 进行进一步细化，同时自动采样推理路径进行学习；这与 RLHF 的不同之处在于它不利用从人类标记数据中学习的奖励模型； ReFT 展示了解决数学问题的性能和泛化能力的提高。

![](pp001/ReFT.png)

增强大型语言模型（LLMs）推理能力的一种方法是使用思想链（CoT）注释进行监督微调（SFT）。然而，这种方法没有表现出足够强的泛化能力，因为训练仅依赖于给定的 CoT 数据。例如，在数学问题解决中，训练数据中的每个问题通常只有一个带注释的推理路径。直观上，算法最好从给定问题的多个带注释的推理路径中学习。为了解决这个问题，我们提出了一种简单而有效的方法，称为强化微调（ReFT），以增强推理学习LLMs的泛化性，以解决数学问题为例。 ReFT首先用SFT预热模型，然后采用在线强化学习，特别是本文中的PPO算法，进一步微调模型，在给定问题的情况下自动采样大量推理路径，自然而然地获得奖励从真实答案中得出。对 GSM8K、MathQA 和 SVAMP 数据集的大量实验表明，ReFT 的性能明显优于 SFT，并且通过结合多数投票和重新排序等推理时间策略，可以进一步提高性能。请注意，ReFT 通过从与 SFT 相同的训练问题中学习来获得改进，而不依赖于额外或增强的训练问题。这表明 ReFT 具有卓越的泛化能力。

论文的主要贡献包括：

- 提出了ReFT方法，它利用强化学习来解决数学问题，并在相同的数据集上展示了比传统SFT更强的泛化能力。
- 在三个标准的数学数据集（GSM8K、MathQA和SVAMP）上进行了广泛的实验，使用了两种基础模型（CodeLLAMA和Galactica），涵盖了自然语言和基于程序的推理链（CoT），证明了ReFT在性能和泛化能力上的显著提升。
- 展示了ReFT在推理时结合多数投票（majority voting）和奖励模型重排（reward model reranking）等策略可以进一步提高性能。

实验结果表明，ReFT在多个数据集上显著优于SFT，并且在不依赖额外或增强训练数据的情况下，通过学习从相同的训练问题中获得改进。这表明ReFT具有优越的泛化能力。此外，论文还讨论了ReFT在训练效率和奖励黑客（reward hacking）方面的局限性，并提出了未来的研究方向，包括使用离线强化学习技术、开发无需预热的方法以及在更广泛的推理任务中应用ReFT。

## Leveraging Large Language Models for NLG Evaluation: A Survey

**作者:** Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.07103

**代码地址：** 无

### 内容概要

评估概述LLMs - 彻底调查方法并探索其优点和局限性；提供了涉及快速工程或校准开源LLMs以进行评估的不同方法的分类。

在快速发展的自然语言生成（NLG）评估领域，引入大型语言模型（LLMs）为评估生成的内容质量（例如连贯性、创造力和上下文相关性）开辟了新途径。本调查旨在全面概述如何利用 LLMs 进行 NLG 评估，这是一个缺乏系统分析的新兴领域。我们提出了一个连贯的分类法来组织现有的基于LLM的评估指标，提供一个结构化框架来理解和比较这些方法。我们的详细探索包括批判性评估各种基于 LLM 的方法，以及比较它们在评估 NLG 输出方面的优势和局限性。通过讨论尚未解决的挑战，包括偏差、稳健性、领域特异性和统一评估，本次调查旨在为研究人员提供见解，并倡导更公平、更先进的 NLG 评估技术。

使用 LLMs 进行评估有很多创造性的方法。下面的分类很好地总结了不同的方法，例如涉及即时工程或校准开源 LLMs 进行评估的方法。

![alt text](pp001/NLGEvaluation.png)

论文的主要贡献包括：

- 提出了一个系统的分类法，用于组织现有的基于LLM的评估指标，为理解和比较这些方法提供了结构化的框架。
- 对各种基于LLM的方法进行了详细探索，比较了它们在评估NLG输出时的优缺点。
- 讨论了尚未解决的挑战，包括偏见、鲁棒性、领域特定性和统一评估，旨在为研究人员提供洞察，并倡导更公平、更先进的NLG评估技术。

论文首先介绍了NLG评估的背景和重要性，然后提出了一个正式的框架来描述LLM基于NLG评估任务。接着，论文详细讨论了基于提示（prompt-based）和基于调优（tuning-based）的评估方法，并对这些方法进行了分类，包括基于分数的评估、概率评估、Likert风格评估、成对比较、集成评估和高级评估协议。

此外，论文还介绍了用于评估LLM基于评估器有效性的元评估基准，并识别了未来研究的潜在开放问题。最后，论文总结了LLM在NLG评估中的作用，并强调了在这一快速发展领域中解决现有问题的重要性。


## Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models

**作者:** Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.06102

**代码地址：** 无

### 内容概要

Patchscopes - 提出了一个利用模型本身来解释其内部表示的框架；它从 LLM 隐藏表示中解码信息，这可以通过将表示“修补”到一个单独的推理过程中来实现，从而实现信息的提取；它可以用来回答有关 LLM 计算的问题，甚至可以用来修复潜在的多跳推理错误。

![](pp001/Patchscopes.png)

Patchscopes是一个统一的框架，用于检查大型语言模型（LLMs）隐藏表示中的信息。Patchscopes的核心思想是利用LLMs生成类似人类的文本的能力，来解释其内部表示的信息。这个框架可以配置为查询LLM表示中的各种信息，并且可以看作是将现有的解释方法（如基于词汇空间投影和计算干预的方法）作为其特例。

主要贡献和特点包括：

- 模块化框架：Patchscopes提供了一个模块化的框架，可以通过配置不同的目标提示（target prompt）、模型和转换函数来检查LLM的计算。
- 统一解释方法：展示了如何将现有的解释方法，如词汇投影和计算干预，视为Patchscopes的实例。
- 新的检查方法：引入了新的配置，提供了更有效的工具来解决相同的问题，同时缓解了现有方法的局限性。
- 跨模型检查：展示了如何使用更强大的模型来解释较小模型的表示，这在以前的工作中没有被探索过。
- 实际应用：展示了Patchscopes在实际应用中的潜力，例如在多跳推理中纠正错误。
- 实验验证：通过一系列实验验证了Patchscopes的好处和机会，特别是在自回归LLMs上。
- 新的可能性：Patchscopes不仅能够解释模型的输出预测，还能解码实体的特定属性，并且能够在早期层中分析实体名称的上下文化过程。
- 自我修正：在多跳推理中，Patchscopes可以用来修正模型在处理连接不同推理步骤时的错误。

## The Unreasonable Effectiveness of Easy Training Data for Hard Tasks

**作者:** Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.06751

**代码地址：** https://github.com/allenai/easy-to-hard-generalization

### 内容概要

简单训练数据对困难任务的不合理有效性——表明语言模型通常能够很好地从简单数据到困难数据进行泛化，即从容易到困难的泛化；它认为，即使重点是提高困难数据的性能，对简单数据进行训练也比对困难数据进行训练更好，并表明可扩展的监督问题可能比以前想象的更容易。


![](pp001/hard.png)

这篇论文探讨了在训练数据难以正确标注的情况下，如何训练模型在困难的测试数据上表现良好的问题，这被称为“可扩展的监督问题”（scalable oversight problem）。研究者们发现，当前的语言模型（LMs）通常能够从简单的数据（easy data）泛化到困难的数据（hard data），并且在某些情况下，即使是使用简单的训练方法（如上下文学习、线性分类器头和QLoRA），也能与在困难数据上训练的“预言机”模型（oracle models）表现相当。

主要发现和结论包括：

1. **易到难的泛化能力**：研究表明，即使是在只有简单数据可用的情况下，LMs也能在困难的测试数据上表现出色，通常能够恢复70%到100%的监督差距（Supervision Gap Recovered, SGR）。

2. **训练数据的成本效益**：在收集困难数据成本较高或标签噪声较大的情况下，收集和训练简单数据可能是更好的选择，因为简单数据通常更便宜且标签更准确。

3. **模型规模与训练测试难度差距**：研究者们发现，随着模型规模的增加，易到难的泛化能力保持稳健。但是，当训练和测试数据之间的难度差距足够大时，易到难的性能可能会开始下降。

4. **多种难度度量**：研究者们使用了多种人类和模型基础的难度度量方法，包括教育/年级水平、专家评分、所需认知技能、问题和答案的字数、组合步骤以及最小描述长度（MDL）。

5. **实验设置**：实验使用了不同规模的Llama-2模型（7b、13b和70b），并在四个公开可用的问答数据集上进行，问题难度从小学三年级的科学问题到大学水平的STEM问题和一般知识常识。

6. **训练方法**：研究者们使用了上下文学习（ICL）、线性分类器头和QLoRA等训练方法，并考虑了是否使用思维链（chain-of-thought, CoT）推理。

7. **无监督基线**：作为无监督基线，研究者们采用了零样本提示（zero-shot prompting），即在没有训练数据的情况下，直接让模型回答问题。

8. **统计测试**：为了获得置信区间和比较结果的p值，研究者们采用了块自举（block bootstrap）方法来结合测试和训练数据的方差。


## MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts

**作者:**  Maciej Pióro, Kamil Ciebiera, Krystian Król, Jan Ludziejewski, Michał Krutul, Jakub Krajewski, Szymon Antoniak, Piotr Miłoś, Marek Cygan, Sebastian Jaszczur

**发表刊物/会议：** arXiv

**发表年份：** 2024

**论文地址：** https://arxiv.org/abs/2401.04081

**代码地址：** https://github.com/kyegomez/MoE-Mamba

### 内容概要

一种通过将状态空间模型 (SSM) 与专家混合 (MoE) 相结合来有效扩展 LLMs 的方法； MoE-Mamba，优于 Mamba 和 Transformer-MoE；它以减少 2.2 倍的训练步骤达到与 Mamba 相同的性能，同时保留了 Mamba 相对于 Transformer 的推理性能增益。

![alt text](pp001/MoE-Mamba.png)

Mamba因其高效的并行扫描和硬件感知设计而在语言模型领域表现出色，而MoE则是一种允许模型参数数量大幅增加而不影响计算需求的技术。研究者们提出，将SSM与MoE结合可以解锁SSM在扩展性方面的潜力。

主要贡献和发现包括：

1. **MoE-Mamba模型**：通过在Mamba模型中引入MoE层，MoE-Mamba在保持推理性能的同时，训练效率得到了显著提升。具体来说，MoE-Mamba在训练步骤上比Mamba减少了2.35倍，同时达到了相同的性能。

2. **实验验证**：通过在不同模型大小、设计选择和专家数量上的全面研究，作者证实了MoE-Mamba的性能提升是稳健的。

3. **模型架构**：MoE-Mamba的架构在Mamba层和MoE层之间交替，这样可以有效地区分每个令牌的无条件处理（由Mamba层处理）和条件处理（由MoE层处理）。

4. **替代设计**：除了将MoE层与Mamba层交替之外，作者还探索了其他设计，例如将MoE层与Mamba层并行执行，以及在Mamba块内集成MoE计算。

5. **未来工作和局限性**：作者提出了未来的研究方向，包括进一步扩展模型规模、探索不同类型的MoE在MoE-Mamba中的最优设计、研究MoE层的蒸馏问题、以及探索Mamba和注意力机制的结合。

6. **结论**：MoE-Mamba为结合MoE和SSM开辟了新的研究方向，有望实现更高效的语言模型扩展。
